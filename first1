import requests
from bs4 import BeautifulSoup
import re
def is_valid_wikipedia_link(url):
    pattern = re.compile(r'^https:\/\/en\.wikipedia\.org\/wiki\/.+')
    return bool(pattern.match(url))

def get_wikipedia_links(url):
    try:
        response = requests.get(url)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        links = set()
        for link in soup.find_all('a', href=True):
            href = link['href']
            if href.startswith('/wiki/') and ':' not in href:
                full_url = 'https://en.wikipedia.org' + href
                links.add(full_url)
            if len(links) >= 10:
                break
        return list(links)
    except requests.exceptions.RequestException as e:
        print(f"Error fetching the URL: {e}")
        return []

def scrape_wikipedia_links(initial_url, cycles):
    if not is_valid_wikipedia_link(initial_url):
        raise ValueError("The provided link is not a valid Wikipedia link.")
    
    if not isinstance(cycles, int) or not (1 <= cycles <= 3):
        raise ValueError("The cycle count must be an integer between 1 and 3.")
    
    all_links = set()
    current_links = get_wikipedia_links(initial_url)
    
    for _ in range(cycles):
        new_links = set()
        for link in current_links:
            if link not in all_links:
                all_links.add(link)
                new_links.update(get_wikipedia_links(link))
        current_links = new_links
    
    return list(all_links)

if __name__ == "__main__":
    initial_url = input("Enter a Wikipedia link: ")
    cycles = int(input("Enter the number of cycles (1-3): "))
    
    try:
        result_links = scrape_wikipedia_links(initial_url, cycles)
        print("Scraped links:")
        for link in result_links:
            print(link)
    except ValueError as e:
        print(e)
